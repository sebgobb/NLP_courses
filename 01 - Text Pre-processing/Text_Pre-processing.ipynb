{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d4aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/seb/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution for Mac issue  only\n",
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636a2692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/seb/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/seb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/seb/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "      <th>Case Normalization</th>\n",
       "      <th>Noise Removal</th>\n",
       "      <th>Tokenization</th>\n",
       "      <th>Stopwords</th>\n",
       "      <th>Stemming</th>\n",
       "      <th>Lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UpgrdCentre Orange customer, you may now claim...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>upgrdcentre orange customer, you may now claim...</td>\n",
       "      <td>upgrdcentre orange customer you may now claim ...</td>\n",
       "      <td>[upgrdcentre, orange, customer, you, may, now,...</td>\n",
       "      <td>[upgrdcentre, orange, customer, may, claim, fr...</td>\n",
       "      <td>[upgrdcentr, orang, custom, you, may, now, cla...</td>\n",
       "      <td>[upgrdcentre, orange, customer, you, may, now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Loan for any purpose £500 - £75,000. Homeowner...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>loan for any purpose £500 - £75,000. homeowner...</td>\n",
       "      <td>loan for any purpose £ £ homeowners tenants we...</td>\n",
       "      <td>[loan, for, any, purpose, £, £, homeowners, te...</td>\n",
       "      <td>[loan, purpose, £, £, homeowners, tenants, wel...</td>\n",
       "      <td>[loan, for, ani, purpos, £, £, homeown, tenant...</td>\n",
       "      <td>[loan, for, any, purpose, £, £, homeowner, ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Congrats! Nokia 3650 video camera phone is you...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>congrats! nokia 3650 video camera phone is you...</td>\n",
       "      <td>congrats nokia video camera phone is your call...</td>\n",
       "      <td>[congrats, nokia, video, camera, phone, is, yo...</td>\n",
       "      <td>[congrats, nokia, video, camera, phone, call, ...</td>\n",
       "      <td>[congrat, nokia, video, camera, phone, is, you...</td>\n",
       "      <td>[congrats, nokia, video, camera, phone, is, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>URGENT! Your Mobile number has been awarded wi...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>urgent! your mobile number has been awarded wi...</td>\n",
       "      <td>urgent your mobile number has been awarded wit...</td>\n",
       "      <td>[urgent, your, mobile, number, has, been, awar...</td>\n",
       "      <td>[urgent, mobile, number, awarded, £, prize, gu...</td>\n",
       "      <td>[urgent, your, mobil, number, ha, been, award,...</td>\n",
       "      <td>[urgent, your, mobile, number, ha, been, award...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Someone has contacted our dating service and e...</td>\n",
       "      <td>Spam</td>\n",
       "      <td>someone has contacted our dating service and e...</td>\n",
       "      <td>someone has contacted our dating service and e...</td>\n",
       "      <td>[someone, has, contacted, our, dating, service...</td>\n",
       "      <td>[someone, contacted, dating, service, entered,...</td>\n",
       "      <td>[someon, ha, contact, our, date, servic, and, ...</td>\n",
       "      <td>[someone, ha, contacted, our, dating, service,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S. No.                                       Message_body Label  \\\n",
       "0       1  UpgrdCentre Orange customer, you may now claim...  Spam   \n",
       "1       2  Loan for any purpose £500 - £75,000. Homeowner...  Spam   \n",
       "2       3  Congrats! Nokia 3650 video camera phone is you...  Spam   \n",
       "3       4  URGENT! Your Mobile number has been awarded wi...  Spam   \n",
       "4       5  Someone has contacted our dating service and e...  Spam   \n",
       "\n",
       "                                  Case Normalization  \\\n",
       "0  upgrdcentre orange customer, you may now claim...   \n",
       "1  loan for any purpose £500 - £75,000. homeowner...   \n",
       "2  congrats! nokia 3650 video camera phone is you...   \n",
       "3  urgent! your mobile number has been awarded wi...   \n",
       "4  someone has contacted our dating service and e...   \n",
       "\n",
       "                                       Noise Removal  \\\n",
       "0  upgrdcentre orange customer you may now claim ...   \n",
       "1  loan for any purpose £ £ homeowners tenants we...   \n",
       "2  congrats nokia video camera phone is your call...   \n",
       "3  urgent your mobile number has been awarded wit...   \n",
       "4  someone has contacted our dating service and e...   \n",
       "\n",
       "                                        Tokenization  \\\n",
       "0  [upgrdcentre, orange, customer, you, may, now,...   \n",
       "1  [loan, for, any, purpose, £, £, homeowners, te...   \n",
       "2  [congrats, nokia, video, camera, phone, is, yo...   \n",
       "3  [urgent, your, mobile, number, has, been, awar...   \n",
       "4  [someone, has, contacted, our, dating, service...   \n",
       "\n",
       "                                           Stopwords  \\\n",
       "0  [upgrdcentre, orange, customer, may, claim, fr...   \n",
       "1  [loan, purpose, £, £, homeowners, tenants, wel...   \n",
       "2  [congrats, nokia, video, camera, phone, call, ...   \n",
       "3  [urgent, mobile, number, awarded, £, prize, gu...   \n",
       "4  [someone, contacted, dating, service, entered,...   \n",
       "\n",
       "                                            Stemming  \\\n",
       "0  [upgrdcentr, orang, custom, you, may, now, cla...   \n",
       "1  [loan, for, ani, purpos, £, £, homeown, tenant...   \n",
       "2  [congrat, nokia, video, camera, phone, is, you...   \n",
       "3  [urgent, your, mobil, number, ha, been, award,...   \n",
       "4  [someon, ha, contact, our, date, servic, and, ...   \n",
       "\n",
       "                                       Lemmatization  \n",
       "0  [upgrdcentre, orange, customer, you, may, now,...  \n",
       "1  [loan, for, any, purpose, £, £, homeowner, ten...  \n",
       "2  [congrats, nokia, video, camera, phone, is, yo...  \n",
       "3  [urgent, your, mobile, number, ha, been, award...  \n",
       "4  [someone, ha, contacted, our, dating, service,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "\n",
    "##### CSV READING with PANDAS\n",
    "\n",
    "df = pd.read_csv(\"SMS_test.csv\", encoding=\"unicode_escape\")\n",
    "df.head()\n",
    "column = \"Message_body\"\n",
    "content = df[column]\n",
    "\n",
    "\n",
    "##### NLTK POINTS\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "##### A. CASE NORMALIZATION\n",
    "\n",
    "## Lowercasing\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "##### B. NOISE REMOVAL\n",
    "\n",
    "## Removing numbers/digits\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r\"\\d+\", \"\", text)\n",
    "    return result\n",
    "\n",
    "## Removing punctuation & special characters\n",
    "punct_list = list(string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    for punct in punct_list:\n",
    "        text = text.replace(punct, \"\")\n",
    "    return text.strip() # remove leading and trialing whitespaces\n",
    "\n",
    "## Removing double whitespaces\n",
    "def remove_doubleWhitespaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "## Removing URLS\n",
    "url_pattern=[]\n",
    "url_pattern.append(re.compile(r'https:?//\\S+'))\n",
    "url_pattern.append(re.compile(r'http:?//\\S+'))\n",
    "url_pattern.append(re.compile(r'www.\\S+'))\n",
    "\n",
    "def remove_urls(text):\n",
    "    for pattern in url_pattern:\n",
    "        text = pattern.sub(\"\",text)\n",
    "    return text\n",
    "\n",
    "\n",
    "##### C. TOKENIZATION\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "##### D. STOPWORDS REMOVAL\n",
    "def remove_stopwords(list_words):\n",
    "    purged_list = []\n",
    "    for word in list_words:\n",
    "        if word.lower() not in stop_words:\n",
    "            purged_list.append(word)\n",
    "    return purged_list\n",
    "\n",
    "\n",
    "##### E-1 STEMMING\n",
    "def stemming(list_words):\n",
    "    stems = []\n",
    "    for word in list_words:\n",
    "        stems.append(stemmer.stem(word))\n",
    "    return stems\n",
    "\n",
    "##### E-2 LEMMATIZATION\n",
    "def func_lemmatize(list_words):\n",
    "    lemmas = []\n",
    "    for word in list_words:\n",
    "        lemmas.append(lemmatizer.lemmatize(word))\n",
    "    return lemmas\n",
    "\n",
    "##### ALL FUNCTIONS in one list\n",
    "\n",
    "NR_functions = [\n",
    "    remove_numbers,\n",
    "    remove_punctuation,\n",
    "    remove_doubleWhitespaces,\n",
    "    remove_urls\n",
    "]\n",
    "\n",
    "##### DATAFRAME\n",
    "\n",
    "# Column 1\n",
    "df[\"Case Normalization\"] = df[column].apply(text_lowercase)\n",
    "\n",
    "# Column 2\n",
    "df[\"Noise Removal\"] = df[\"Case Normalization\"] # In-between column to apply all NR functions\n",
    "for funct in NR_functions:\n",
    "    df[\"Noise Removal\"] = df[\"Noise Removal\"].apply(funct)\n",
    "\n",
    "# Column 3\n",
    "df[\"Tokenization\"] = df[\"Noise Removal\"].apply(tokenize)\n",
    "\n",
    "# Column 4\n",
    "df[\"Stopwords\"] = df[\"Tokenization\"].apply(remove_stopwords)\n",
    "\n",
    "# Column 5\n",
    "df[\"Stemming\"] = df[\"Tokenization\"].apply(stemming)\n",
    "\n",
    "# Column 5\n",
    "df[\"Lemmatization\"] = df[\"Tokenization\"].apply(func_lemmatize)\n",
    "\n",
    "\n",
    "##### DISPLAYING\n",
    "display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
